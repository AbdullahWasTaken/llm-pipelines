{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07468b0",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6597d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field, InitVar\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_id: str                                = field(metadata={\"help\": \"model name on HuggingFace or path to local model\"})\n",
    "    adapter_name: Optional[str]                  = field(default=None, metadata={\"help\": \"adapter name on HuggingFace or path to local adapter\"})\n",
    "    load_in_4bit: InitVar[bool]                  = field(default=True, metadata={\"help\": \"enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from bitsandbytes\"})\n",
    "    bnb_4bit_compute_dtype: InitVar[torch.dtype] = field(default=torch.bfloat16, metadata={\"help\": \"Computational type: Union[torch.bfloat16, torch.float16, torch.float32]\"})\n",
    "    bnb_4bit_quant_type: InitVar[str]            = field(default=\"nf4\", metadata={\"help\": \"quantization data type in the bnb.nn.Linear4Bit layers: Union['nf4', 'fp4']\"})\n",
    "    bnb_4bit_use_double_quant: InitVar[bool]     = field(default=False, metadata={\"help\": \"enable nested quantization\"})\n",
    "    quant_config: BitsAndBytesConfig             = field(init=False)\n",
    "    device_map: str                              = field(default=\"auto\")\n",
    "    output_hidden_states: InitVar[bool]          = field(default=False, metadata={\"help\": \"outputs hidden states (W) during fwd pass\"})\n",
    "    output_attentions: InitVar[bool]             = field(default=False, metadata={\"help\": \"outputs attentions calculated during fwd pass\"})\n",
    "    output_scores: InitVar[bool]                 = field(default=False, metadata={\"help\": \"outputs logits calculated during fwd pass\"})\n",
    "    return_dict_in_generate: InitVar[bool]       = field(default=False, metadata={\"help\": \"return ModelOutput during generation or a simple tuple\"})\n",
    "    config_args: dict                            = field(init=False)\n",
    "        \n",
    "    def __post_init__(self, load_in_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant,\n",
    "                      output_hidden_states, output_attentions, output_scores, return_dict_in_generate):\n",
    "        self.quant_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit,\n",
    "                                               bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "                                               bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "                                               bnb_4bit_use_double_quant=bnb_4bit_use_double_quant)\n",
    "        \n",
    "        self.config_args = {\"output_hidden_states\":output_hidden_states,\n",
    "                            \"output_attentions\":output_attentions,\n",
    "                            \"output_scores\": output_scores,\n",
    "                            \"return_dict_in_generate\": return_dict_in_generate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28010e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_args = ModelArguments(\"facebook/opt-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_args.model_id,\n",
    "                                             quantization_config=model_args.quant_config,\n",
    "                                             device_map=model_args.device_map,\n",
    "                                             **model_args.config_args)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_id, device_map=model_args.device_map)\n",
    "\n",
    "# embedding size check\n",
    "# idk why this is done (copied directly from HF script)\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    print(\"Resizing embeddings to avoid index errors\")\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0602a0",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37518171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PeftArguments:\n",
    "    lora_r: int                                    = field(metadata={\"help\": \"rank of the update matrices\"})\n",
    "    lora_alpha: int                                = field(metadata={\"help\": \"alpha parameter for Lora scaling\"})\n",
    "    lora_dropout: float                            = field(metadata={\"help\": \"dropout probability for Lora layers\"})\n",
    "    target_modules: List[str]                      = field(metadata={\"help\": \"names of the modules to apply Lora to\"})\n",
    "    bias: str                                      = field(default=\"none\", metadata={\"help\": \"Bias type for Lora. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the corresponding biases will be updated during training. Be aware that this means that, even when disabling the adapters, the model will not produce the same output as the base model would have without adaptation\"})\n",
    "    modules_to_save: Optional[List[str]]           = field(default=None, metadata={\"help\": \"List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model’s custom head that is randomly initialized for the fine-tuning task\"})\n",
    "    layers_to_transform: Optional[List[int] | int] = field(default=None, metadata={\"help\": \"List of layers to be transformed by LoRA. If not specified, all layers in target_modules are transformed\"})\n",
    "\n",
    "peft_args = PeftArguments(lora_r=16, lora_alpha=64, lora_dropout=0.05, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"])\n",
    "\n",
    "config = LoraConfig(r=peft_args.lora_r,\n",
    "                    lora_alpha=peft_args.lora_alpha,\n",
    "                    lora_dropout=peft_args.lora_dropout,\n",
    "                    bias=peft_args.bias,\n",
    "                    target_modules=peft_args.target_modules,\n",
    "                    modules_to_save=peft_args.modules_to_save,\n",
    "                    layers_to_transform=peft_args.layers_to_transform,\n",
    "                    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, config)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce819c8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5109227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "@dataclass\n",
    "class DatasetArguments:\n",
    "    dataset_id: str = field(metadata={\"help\": \"dataset name on HuggingFace or path to Union[datasets.Dataset, csv, pandas.DataFrame, dict]\"})\n",
    "    valid_split_name: str = field(default=\"valid\")\n",
    "    train_split_name: str = field(default=\"train\")\n",
    "    validation_split_percentage: int = field(default=10)\n",
    "    preprocessing_func: Optional[Callable] = field(default=None)\n",
    "    num_workers: int = field(default=1)\n",
    "    text_column_name: str = field(default=\"text\")\n",
    "    block_size: int = field(default=1024)\n",
    "    postprocessing_func: Optional[Callable] = field(default=None)\n",
    "    max_train_samples: Optional[int] = field(default=None)\n",
    "    max_eval_samples: Optional[int] = field(default=None)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        if self.validation_split_percentage > 100:\n",
    "            self.validation_split_percentage = self.validation_split_percentage%100\n",
    "\n",
    "\n",
    "dataset_args = DatasetArguments(\"Abirate/english_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b75f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "if dataset_args.dataset_id.endswith(\".csv\"):\n",
    "    raw_datasets = Dataset.from_csv(dataset_args.dataset_id)\n",
    "elif isinstance(dataset_args.dataset_id, dict):\n",
    "    raw_datasets = Dataset.from_dict(dataset_args.dataset_id)\n",
    "elif isinstance(dataset_args.dataset_id, pd.DataFrame):\n",
    "    raw_datasets = Dataset.from_pandas(dataset_args.dataset_id)\n",
    "else:\n",
    "    raw_datasets = load_dataset(dataset_args.dataset_id)\n",
    "\n",
    "if dataset_args.valid_split_name not in raw_datasets.keys():\n",
    "    raw_datasets[dataset_args.valid_split_name] = load_dataset(dataset_args.dataset_id, split=f\"train[:{dataset_args.validation_split_percentage}%]\")\n",
    "    raw_datasets[dataset_args.train_split_name] = load_dataset(dataset_args.dataset_id, split=f\"train[{dataset_args.validation_split_percentage}%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b4f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if callable(dataset_args.preprocessing_func):\n",
    "    raw_datasets = raw_datasets.map(dataset_args.preprocessing_func, batched=True, num_proc=dataset_args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20d854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(raw_datasets[dataset_args.train_split_name].features)\n",
    "dataset_args.text_column_name = column_names[0] if dataset_args.text_column_name not in column_names else dataset_args.text_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe3832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args.block_size = min(dataset_args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[dataset_args.text_column_name])\n",
    "    return output\n",
    "\n",
    "raw_datasets = raw_datasets.map(tokenize_function, batched=True, num_proc=dataset_args.num_workers, remove_columns=column_names)\n",
    "raw_datasets = raw_datasets.filter(lambda example: len(example['input_ids']) < dataset_args.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61492df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if callable(dataset_args.postprocessing_func):\n",
    "    raw_datasets = raw_datasets.map(dataset_args.postprocessing_func, batched=True, num_proc=dataset_args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e63cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[dataset_args.train_split_name]\n",
    "if dataset_args.max_train_samples is not None:\n",
    "    dataset_args.max_train_samples = min(len(train_dataset), dataset_args.max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(dataset_args.max_train_samples))\n",
    "\n",
    "eval_dataset = raw_datasets[dataset_args.valid_split_name]\n",
    "if dataset_args.max_eval_samples is not None:\n",
    "    dataset_args.max_eval_samples = min(len(eval_dataset), dataset_args.max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(dataset_args.max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629a24e",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec99824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # Masked language modeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "# # SFT\n",
    "# data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=\"### Response:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7421a",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04cb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    output_dir=\"./results\", # output directory where the model predictions and checkpoints are saved\n",
    "    # overwrite_output_dir=False, # overwrite the content of the output directory\n",
    "    evaluation_strategy=\"epoch\", # \"no\": No evaluation , \"steps\": Evaluation done (and logged) every eval_steps, \"epoch\": Evaluation done end of each epoch\n",
    "    # eval_steps=50,\n",
    "    # prediction_loss_only=False, # When performing evaluation and generating predictions, only returns the loss\n",
    "    per_device_train_batch_size=1, # The batch size per GPU/TPU core/CPU for training\n",
    "    # per_device_eval_batch_size=8, # The batch size per GPU/TPU core/CPU for evaluation\n",
    "    # gradient_accumulation_steps=1, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass \n",
    "                                   # When using gradient accumulation, one step is counted as one step with backward pass\n",
    "    # eval_accumulation_steps=None, # Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. \n",
    "                                  # If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but requires more memory)\n",
    "    # eval_delay=0 # Number of epochs or steps to wait for before the first evaluation can be performed, depending on the evaluation_strategy\n",
    "    learning_rate=2e-4, # The initial learning rate for AdamW optimizer\n",
    "    # weight_decay=0, # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer\n",
    "    # adam_beta1=0.9, # The beta1 hyperparameter for the AdamW optimizer\n",
    "    # adam_beta2=0.99, # The beta2 hyperparameter for the AdamW optimizer\n",
    "    # adam_epsilon=1e-8, # The epsilon hyperparameter for the AdamW optimizer\n",
    "    max_grad_norm=0.3, # Maximum gradient norm (for gradient clipping)\n",
    "    # num_train_epochs=3.0, # Total number of training epochs to perform (perform decimal part percents if non-int)\n",
    "    max_steps=100, # default -1, If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. \n",
    "                  # In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted\n",
    "    lr_scheduler_type=\"constant\", # one of \"linear\" (default) , \"constant\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant_with_warmup\", \"inverse_sqrt\", \"reduce_lr_on_plateau\"\n",
    "    warmup_ratio=0.03, # default 0, Ratio of total training steps used for a linear warmup from 0 to learning_rate\n",
    "    # warmup_steps=0, # Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio\n",
    "    # log_level=\"passive\", # one of 'debug', 'info', 'warning', 'error' and 'critical', 'passive' defaults to transformers level (\"warning\")\n",
    "    # log_level_replica=\"warning\", # Logger log level to use on replicas\n",
    "    # log_on_each_node=True, # In multinode distributed training, whether to log using log_level once per node, or only on the main node\n",
    "    # logging_dir=None, #  TensorBoard log directory. Will default to *output_dir/runs/CURRENT_DATETIME_HOSTNAME*\n",
    "    # logging_strategy=\"steps\", # logging strategy to adopt during training. One of \"no\", \"epoch\", \"steps\"\n",
    "    # logging_first_step=False, # Whether to log and evaluate the first global_step or not\n",
    "    logging_steps=5, # Number of update steps between two logs if logging_strategy=\"steps\". \n",
    "                      # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps\n",
    "    # logging_nan_inf_filter=True, # Whether to filter nan and inf losses for logging. only influences logging, not the behavior the gradient\n",
    "    # save_strategy=\"steps\", # save strategy. One of \"no\", \"epoch\", \"steps\"\n",
    "    save_steps=5, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "                   # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
    "    # save_total_limit=None, # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "                           # When load_best_model_at_end is enabled, the “best” checkpoint according to metric_for_best_model will always be retained in addition to the most recent ones\n",
    "    # save_safetensors=False, # Use safetensors saving and loading for state dicts instead of default torch.load and torch.save.\n",
    "    # save_on_each_node=False, # When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on the main one.\n",
    "    # no_cuda=False, # Whether to not use CUDA even when it is available or not.\n",
    "    # seed=42, # Random seed that will be set at the beginning of training.\n",
    "    # data_seed=None, # Random seed to be used with data samplers. If not set, random generators for data sampling will use the same seed as seed.\n",
    "    # jit_mode_eval=False, # Whether or not to use PyTorch jit trace for inference.\n",
    "    # use_ipex=False, # Use Intel extension for PyTorch when it is available\n",
    "    # bf16=False, # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    # fp16=False, # default False, Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    # fp16_opt_level=\"O1\", # For fp16 training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’].\n",
    "    # fp16_backend=\"auto\", # This argument is deprecated. Use half_precision_backend instead.\n",
    "    # half_precision_backend=\"auto\", # The backend to use for mixed precision training. one of \"auto\", \"cuda_amp\", \"apex\", \"cpu_amp\"\n",
    "    # bf16_full_eval=False, # Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm metric values\n",
    "    # fp16_full_eval=False, # Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm metric values.\n",
    "    # tf32=None, # Whether to enable the TF32 mode, available in Ampere and newer GPU architectures.\n",
    "    # local_rank=-1, # Rank of the process during distributed training\n",
    "    # ddp_backend=None, # The backend to use for distributed training. Must be one of \"nccl\", \"mpi\", \"ccl\", \"gloo\".\n",
    "    # tpu_num_cores=None, # When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
    "    # dataloader_drop_last=False, # Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\n",
    "    # eval_steps=None, # Number of update steps between two evaluations if evaluation_strategy=\"steps\"\n",
    "    # dataloader_num_workers=0, # Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process.\n",
    "    # past_index=-1, # Some models like TransformerXL or XLNet can make use of the past hidden states for their predictions\n",
    "    # run_name=None, # A descriptor for the run. Typically used for wandb and mlflow logging.\n",
    "    # disable_tqdm=None, # Whether or not to disable the tqdm progress bars and table of metrics produced by ~notebook.NotebookTrainingTracker in Jupyter Notebooks\n",
    "    remove_unused_columns=False, # Whether or not to automatically remove the columns unused by the model forward method.\n",
    "    # label_names=None, # The list of keys in your dictionary of inputs that correspond to the labels.\n",
    "    # load_best_model_at_end=False, # Whether or not to load the best model found during training at the end of training\n",
    "    # metric_for_best_model=None, # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models\n",
    "    # greater_is_better=None, # specify if better models should have a greater metric or not\n",
    "    # ignore_data_skip=False, # When resuming training, whether or not to skip the epochs and batches to get the data loading at the same stage as in the previous training\n",
    "    # sharded_ddp=False, # Use Sharded DDP training from FairScale (in distributed training only)\n",
    "    # fsdp=False, # Use PyTorch Distributed Parallel Training\n",
    "    # fsdp_config=None, # Config to be used with fsdp\n",
    "    # deepspeed=None, # Use Deepspeed\n",
    "    # label_smoothing_factor=0.0, # The label smoothing factor to use\n",
    "    # debug=\"\", # Enable one or more debug features\n",
    "    # optim=\"paged_adamw_32bit\", # default \"adamw_hf\", The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n",
    "    # optim_args=None, # Optional arguments that are supplied to AnyPrecisionAdamW\n",
    "    # group_by_length=False, # Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding\n",
    "    # length_column_name=\"length\", # Column name for precomputed lengths. If the column exists, grouping by length will use these values rather than computing them on train startup\n",
    "    # report_to=\"all\", # The list of integrations to report the results and logs to. \n",
    "                     # Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\".\n",
    "                     # Use \"all\" to report to all integrations installed, \"none\" for no integrations\n",
    "    # ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True,\n",
    "    # push_to_hub=False,\n",
    "    # resume_from_checkpoint=None, # The path to a folder with a valid checkpoint for your model. \n",
    "                                 # This argument is not directly used by Trainer, it’s intended to be used by your training/evaluation scripts instead.\n",
    "    # hub_model_id=None, hub_strategy=\"every_save\", hub_token=None, hub_private_repo=False, \n",
    "    # gradient_checkpointing=False, # If True, use gradient checkpointing to save memory at the expense of slower backward pass\n",
    "    # include_inputs_for_metrics=False, # Whether or not the inputs will be passed to the compute_metrics function.\n",
    "    # auto_find_batch_size=False, # Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed\n",
    "    # full_determinism=False, torchdynamo=None, ray_scope=\"last\", ddp_timeout=1800, use_mps_device=False,\n",
    "    # torch_compile=False, # Whether or not to compile the model using PyTorch 2.0 torch.compile\n",
    "    # torch_compile_backend=None, torch_compile_mode=None, sortish_sampler=False, \n",
    "    # predict_with_generate=False, # Whether to use generate to calculate generative metrics (ROUGE, BLEU)\n",
    "    # generation_max_length=None, # The max_length to use on each evaluation loop when predict_with_generate=True\n",
    "    # generation_num_beams=None, # The num_beams to use on each evaluation loop when predict_with_generate=True\n",
    "    # generation_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb0047",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e6b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "# metric = evaluate.load(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a69b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    preds[preds == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    return metric.compute(predictions=preds, references=labels) # for accuracy\n",
    "#     return metric.compute(predictions=tokenizer.decode(preds), model_id=model_args.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99aefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics if training_arguments.do_eval else None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics if training_arguments.do_eval else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea53bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d66a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revelo/huggingface_lib/transformers/src/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.459700</td>\n",
       "      <td>2.830065</td>\n",
       "      <td>0.834267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=3.2782231616973876, metrics={'train_runtime': 21.5434, 'train_samples_per_second': 4.642, 'train_steps_per_second': 4.642, 'total_flos': 1156827386880.0, 'train_loss': 3.2782231616973876, 'epoch': 0.04})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350e92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
